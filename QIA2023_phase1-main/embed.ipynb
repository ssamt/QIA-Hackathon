{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "037b62db-a8c4-4bd5-98fb-0b09772c1747",
   "metadata": {
    "id": "037b62db-a8c4-4bd5-98fb-0b09772c1747",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1680321733012,
     "user_tz": -540,
     "elapsed": 6730,
     "user": {
      "displayName": "김병권",
      "userId": "17299946839009927524"
     }
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "import math\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cpu')"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7082881e-2055-47ac-8ac6-dfa2d3944395",
   "metadata": {
    "id": "7082881e-2055-47ac-8ac6-dfa2d3944395",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1680321733489,
     "user_tz": -540,
     "elapsed": 482,
     "user": {
      "displayName": "김병권",
      "userId": "17299946839009927524"
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "       User_ID  Gender  Age  MBTI  Q_number  \\\n11515      240       0   40  ISTJ        44   \n11516      240       0   40  ISTJ        45   \n11517      240       0   40  ISTJ        46   \n11518      240       0   40  ISTJ        47   \n11519      240       0   40  ISTJ        48   \n\n                                                  Answer  \n11515  <그렇다> 저는 계획에 차질이 생기면 돌아가기 위해 노력을 합니다. 이유는 그 계획...  \n11516  <그렇다> 저는 예전의 실수를 후회할 때가 많습니다. 이유는 그만큼 나태하게 산 적...  \n11517  <아니다> 저는 인간의 존재와 삶의 이유에 대해 깊이 생각하지 않습니다. 이유는 이...  \n11518  <아니다> 저는 감정에 휘둘리는 편이 아닙니다. 이유는 감정을 감추고 밖으로 표현하...  \n11519  <아니다> 저는 상대방 잘못일 때 상대방의 체면을 살려주기 위해 노력하지 않습니다....  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>User_ID</th>\n      <th>Gender</th>\n      <th>Age</th>\n      <th>MBTI</th>\n      <th>Q_number</th>\n      <th>Answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>11515</th>\n      <td>240</td>\n      <td>0</td>\n      <td>40</td>\n      <td>ISTJ</td>\n      <td>44</td>\n      <td>&lt;그렇다&gt; 저는 계획에 차질이 생기면 돌아가기 위해 노력을 합니다. 이유는 그 계획...</td>\n    </tr>\n    <tr>\n      <th>11516</th>\n      <td>240</td>\n      <td>0</td>\n      <td>40</td>\n      <td>ISTJ</td>\n      <td>45</td>\n      <td>&lt;그렇다&gt; 저는 예전의 실수를 후회할 때가 많습니다. 이유는 그만큼 나태하게 산 적...</td>\n    </tr>\n    <tr>\n      <th>11517</th>\n      <td>240</td>\n      <td>0</td>\n      <td>40</td>\n      <td>ISTJ</td>\n      <td>46</td>\n      <td>&lt;아니다&gt; 저는 인간의 존재와 삶의 이유에 대해 깊이 생각하지 않습니다. 이유는 이...</td>\n    </tr>\n    <tr>\n      <th>11518</th>\n      <td>240</td>\n      <td>0</td>\n      <td>40</td>\n      <td>ISTJ</td>\n      <td>47</td>\n      <td>&lt;아니다&gt; 저는 감정에 휘둘리는 편이 아닙니다. 이유는 감정을 감추고 밖으로 표현하...</td>\n    </tr>\n    <tr>\n      <th>11519</th>\n      <td>240</td>\n      <td>0</td>\n      <td>40</td>\n      <td>ISTJ</td>\n      <td>48</td>\n      <td>&lt;아니다&gt; 저는 상대방 잘못일 때 상대방의 체면을 살려주기 위해 노력하지 않습니다....</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load datasets\n",
    "df = pd.read_csv('../data/hackathon_train.csv', encoding='cp949', index_col=0)\n",
    "df.index = range(len(df))\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "      User_ID  Gender  Age  MBTI  Q_number Short_Answer  \\\n7195      240  female   30  ISTP        56          그렇다   \n7196      240  female   30  ISTP        57          아니다   \n7197      240  female   30  ISTP        58          아니다   \n7198      240  female   30  ISTP        59          아니다   \n7199      240  female   30  ISTP        60          그렇다   \n\n                                            Long_Answer  \n7195  거래처에 가격 조정 때문에 3군데를 가야 하는 상황이었는데 이야기 잘 통하는 곳 2...  \n7196      상대방과 논쟁을 불러드릴 주제에는 관심이 없습니다 괜히 싸움을 일으키기 싫습니다   \n7197         나에게 온 기회를 포기할 수 없다 양보를 하게 되면 나에게 기회는 없어지니깐  \n7198        마감 기한이 정해지면 그 일을 끝날 때까지 늦게까지 일을 하고 퇴근하곤 합니다  \n7199           일을 할 때는 항상 진행될 때 이외의 상황을 생각하여 대처할 준비를 한다  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>User_ID</th>\n      <th>Gender</th>\n      <th>Age</th>\n      <th>MBTI</th>\n      <th>Q_number</th>\n      <th>Short_Answer</th>\n      <th>Long_Answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>7195</th>\n      <td>240</td>\n      <td>female</td>\n      <td>30</td>\n      <td>ISTP</td>\n      <td>56</td>\n      <td>그렇다</td>\n      <td>거래처에 가격 조정 때문에 3군데를 가야 하는 상황이었는데 이야기 잘 통하는 곳 2...</td>\n    </tr>\n    <tr>\n      <th>7196</th>\n      <td>240</td>\n      <td>female</td>\n      <td>30</td>\n      <td>ISTP</td>\n      <td>57</td>\n      <td>아니다</td>\n      <td>상대방과 논쟁을 불러드릴 주제에는 관심이 없습니다 괜히 싸움을 일으키기 싫습니다</td>\n    </tr>\n    <tr>\n      <th>7197</th>\n      <td>240</td>\n      <td>female</td>\n      <td>30</td>\n      <td>ISTP</td>\n      <td>58</td>\n      <td>아니다</td>\n      <td>나에게 온 기회를 포기할 수 없다 양보를 하게 되면 나에게 기회는 없어지니깐</td>\n    </tr>\n    <tr>\n      <th>7198</th>\n      <td>240</td>\n      <td>female</td>\n      <td>30</td>\n      <td>ISTP</td>\n      <td>59</td>\n      <td>아니다</td>\n      <td>마감 기한이 정해지면 그 일을 끝날 때까지 늦게까지 일을 하고 퇴근하곤 합니다</td>\n    </tr>\n    <tr>\n      <th>7199</th>\n      <td>240</td>\n      <td>female</td>\n      <td>30</td>\n      <td>ISTP</td>\n      <td>60</td>\n      <td>그렇다</td>\n      <td>일을 할 때는 항상 진행될 때 이외의 상황을 생각하여 대처할 준비를 한다</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_excel('../data2/train_data.xlsx', index_col=0)\n",
    "df2.index = range(len(df2))\n",
    "df2.tail()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "       index.1                                           Question  \\\nindex                                                               \n56          56  단계를 건너뛰는 일 없이 절차대로 일을 완수하는 편인가요? 그러한 최근 경험은 어떤...   \n57          57  논란이 되거나 논쟁을 불러일으킬 수 있는 주제에 관심이 많나요? 최근의 사례를 말씀...   \n58          58  자신보다 다른 사람에게 더 필요한 기회라고 생각되면 기회를 포기할 수도 있나요? 이...   \n59          59                     마감 기한을 지키기가 힘든가요? 경험을 이야기해보아요.   \n60          60    일이 원하는 대로 진행될 것이라는 자신감이 있나요? 그렇게 된 계기나 이유가 있나요.   \n\n                                          Positive  \\\nindex                                                \n56                              절차대로 일을 완수하는 편이에요.   \n57             논란이 되거나 논쟁을 불러일으킬 수 있는 주제에 관심이 많아요.   \n58     저보다 다른 사람에게 더 필요한 기회라고 생각되면 기회를 포기할 수도 있어요.   \n59                                    마감 기한은 힘들어요.   \n60                    일이 원하는 대로 진행될 것이라는 자신감이 있어요.   \n\n                                    Negative  \nindex                                         \n56                       단계를 건너뛰는 일도 종종 있어요.  \n57                                       NaN  \n58     저보다 다른 사람에게 더 필요한 기회라 해도 기회는 붙잡아야 해요.  \n59                             마감 기한을 잘 지켜요.  \n60                       일의 진행되는 것에 비관적이에요..  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index.1</th>\n      <th>Question</th>\n      <th>Positive</th>\n      <th>Negative</th>\n    </tr>\n    <tr>\n      <th>index</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>56</th>\n      <td>56</td>\n      <td>단계를 건너뛰는 일 없이 절차대로 일을 완수하는 편인가요? 그러한 최근 경험은 어떤...</td>\n      <td>절차대로 일을 완수하는 편이에요.</td>\n      <td>단계를 건너뛰는 일도 종종 있어요.</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>57</td>\n      <td>논란이 되거나 논쟁을 불러일으킬 수 있는 주제에 관심이 많나요? 최근의 사례를 말씀...</td>\n      <td>논란이 되거나 논쟁을 불러일으킬 수 있는 주제에 관심이 많아요.</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>58</th>\n      <td>58</td>\n      <td>자신보다 다른 사람에게 더 필요한 기회라고 생각되면 기회를 포기할 수도 있나요? 이...</td>\n      <td>저보다 다른 사람에게 더 필요한 기회라고 생각되면 기회를 포기할 수도 있어요.</td>\n      <td>저보다 다른 사람에게 더 필요한 기회라 해도 기회는 붙잡아야 해요.</td>\n    </tr>\n    <tr>\n      <th>59</th>\n      <td>59</td>\n      <td>마감 기한을 지키기가 힘든가요? 경험을 이야기해보아요.</td>\n      <td>마감 기한은 힘들어요.</td>\n      <td>마감 기한을 잘 지켜요.</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>60</td>\n      <td>일이 원하는 대로 진행될 것이라는 자신감이 있나요? 그렇게 된 계기나 이유가 있나요.</td>\n      <td>일이 원하는 대로 진행될 것이라는 자신감이 있어요.</td>\n      <td>일의 진행되는 것에 비관적이에요..</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dq = pd.read_excel('../data/Question.xlsx', index_col=0)\n",
    "dq.tail()"
   ],
   "metadata": {
    "id": "DowFsIq9WrdQ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1680321733939,
     "user_tz": -540,
     "elapsed": 456,
     "user": {
      "displayName": "김병권",
      "userId": "17299946839009927524"
     }
    }
   },
   "id": "DowFsIq9WrdQ"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "load_tuned = False\n",
    "MBTI = ['IE', 'SN', 'TF', 'JP']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e967cc0e-7418-464a-8917-29b59e07412c",
   "metadata": {
    "id": "e967cc0e-7418-464a-8917-29b59e07412c",
    "outputId": "185687b5-7f36-4d83-da84-ec84c5e850d5",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1680321739467,
     "user_tz": -540,
     "elapsed": 5542,
     "user": {
      "displayName": "김병권",
      "userId": "17299946839009927524"
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "if load_tuned:\n",
    "    state = torch.load('tuned/roberta/0.pt', map_location=torch.device('cpu'))\n",
    "    model = AutoModel.from_pretrained(\"klue/roberta-base\").to(device)\n",
    "    model.load_state_dict(state)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-base\")\n",
    "else:\n",
    "    model = AutoModel.from_pretrained(\"klue/roberta-large\").to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "answer_map = {'그렇다':0, '그렇자':0,\n",
    "              '중립':1, '중랍':1, '중간':1, '보통':1, '중립/모르겠다': 1,\n",
    "              '어렵다':2, '아니다':2, '<아니다':2, '아니요':2, '아니오':2}\n",
    "def combine(dfx, filenum):\n",
    "    list = []\n",
    "    for i in range(len(dfx)):\n",
    "        if filenum == 1:\n",
    "            div = dfx.Answer[i].index('>')+1\n",
    "            short_answer = dfx.Answer[i][:div].replace(' ', '')[1:-1]\n",
    "            long_answer = dfx.Answer[i][div:]\n",
    "        elif filenum == 2:\n",
    "            short_answer = dfx.Short_Answer[i]\n",
    "            long_answer = dfx.Long_Answer[i]\n",
    "        answer_code = answer_map[short_answer]\n",
    "        q = dfx.Q_number[i]\n",
    "        prefix = {0:dq.Positive[q], 1:'', 2:dq.Negative[q]}[answer_code]\n",
    "        if type(prefix) == float:\n",
    "            prefix = ''\n",
    "        list.append(prefix+' '+long_answer)\n",
    "    return list"
   ],
   "metadata": {
    "id": "Be7iTihlWrdS",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1680321936373,
     "user_tz": -540,
     "elapsed": 273,
     "user": {
      "displayName": "김병권",
      "userId": "17299946839009927524"
     }
    }
   },
   "id": "Be7iTihlWrdS"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "def long_only(dfx, filenum):\n",
    "    list = []\n",
    "    for i in range(len(dfx)):\n",
    "        if filenum == 1:\n",
    "            div = dfx.Answer[i].index('>')+1\n",
    "            long_answer = dfx.Answer[i][div:]\n",
    "        elif filenum == 2:\n",
    "            long_answer = dfx.Long_Answer[i]\n",
    "        list.append(long_answer)\n",
    "    return list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "comb = combine(df, 1)+combine(df2, 2)\n",
    "long_answers = long_only(df, 1)+long_only(df2, 2)"
   ],
   "metadata": {
    "id": "5uVJb6a1WrdS",
    "outputId": "dcd26e3b-1639-43a4-fafe-c1c967346b1c",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1680321940083,
     "user_tz": -540,
     "elapsed": 609,
     "user": {
      "displayName": "김병권",
      "userId": "17299946839009927524"
     }
    }
   },
   "id": "5uVJb6a1WrdS"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "'일을 할 때는 항상 진행될 때 이외의 상황을 생각하여 대처할 준비를 한다'"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_answers[-1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2364: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "max_length = model.config.max_position_embeddings\n",
    "tensor = tokenizer(comb, max_length=max_length, return_tensors='pt', padding=True)\n",
    "tensor_long = tokenizer(long_answers, max_length=max_length, return_tensors='pt', padding=True)"
   ],
   "metadata": {
    "id": "qERBESf4WrdS",
    "outputId": "00dcf6c5-094b-40af-aa2b-41ed5d88fe3f",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1680321984335,
     "user_tz": -540,
     "elapsed": 2695,
     "user": {
      "displayName": "김병권",
      "userId": "17299946839009927524"
     }
    }
   },
   "id": "qERBESf4WrdS"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 이미 있던 친구들과만 지내요. 어릴 때 왕따 당한 경험이 있고 외부 활동을 좋아하지 않기 때문에 소수의 친구와만 지냅니다. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[CLS] 어릴 때 왕따 당한 경험이 있고 외부 활동을 좋아하지 않기 때문에 소수의 친구와만 지냅니다. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tensor.input_ids[0]))\n",
    "print(tokenizer.decode(tensor_long.input_ids[0]))"
   ],
   "metadata": {
    "id": "DkPSEkW4WrdT",
    "outputId": "162db72e-97eb-4782-beee-93f3c7c89e11",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1680322014848,
     "user_tz": -540,
     "elapsed": 10448,
     "user": {
      "displayName": "김병권",
      "userId": "17299946839009927524"
     }
    }
   },
   "id": "DkPSEkW4WrdT"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "torch.save(tensor, 'roberta/tokens.pt')\n",
    "torch.save(tensor_long, 'roberta/tokens_long.pt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "tensor = torch.load('roberta/tokens.pt').input_ids\n",
    "tensor_long = torch.load('roberta/tokens_long.pt').input_ids"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 일이 원하는 대로 진행될 것이라는 자신감이 있어요. 일을 할 때는 항상 진행될 때 이외의 상황을 생각하여 대처할 준비를 한다 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[CLS]\n",
      "이미\n",
      "있\n",
      "##던\n",
      "친구\n",
      "##들\n",
      "##과\n",
      "##만\n",
      "지내\n",
      "##요\n",
      ".\n",
      "어릴\n",
      "때\n",
      "왕따\n",
      "당한\n",
      "경험\n",
      "##이\n",
      "있\n",
      "##고\n",
      "외부\n",
      "활동\n",
      "##을\n",
      "좋아하\n",
      "##지\n",
      "않\n",
      "##기\n",
      "때문\n",
      "##에\n",
      "소수\n",
      "##의\n",
      "친구\n",
      "##와\n",
      "##만\n",
      "지\n",
      "##냅니다\n",
      ".\n",
      "[SEP]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tensor[-1]))\n",
    "for x in tensor[0]:\n",
    "    print(tokenizer.decode(x))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(31485)"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(tensor)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "589ad9a4-8b31-4249-a9e1-05a49fbeeaa4",
   "metadata": {
    "id": "589ad9a4-8b31-4249-a9e1-05a49fbeeaa4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1680317110962,
     "user_tz": -540,
     "elapsed": 10,
     "user": {
      "displayName": "김병권",
      "userId": "17299946839009927524"
     }
    }
   },
   "outputs": [],
   "source": [
    "class MyMapDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b90b819c-5fdb-4778-84d0-523d26732353",
   "metadata": {
    "id": "b90b819c-5fdb-4778-84d0-523d26732353",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1680317110962,
     "user_tz": -540,
     "elapsed": 8,
     "user": {
      "displayName": "김병권",
      "userId": "17299946839009927524"
     }
    }
   },
   "outputs": [],
   "source": [
    "ds = MyMapDataset(tensor)\n",
    "dl = DataLoader(ds, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d55d08bd-4f8e-4f6a-87ff-c305309e3142",
   "metadata": {
    "id": "d55d08bd-4f8e-4f6a-87ff-c305309e3142",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1680317110962,
     "user_tz": -540,
     "elapsed": 8,
     "user": {
      "displayName": "김병권",
      "userId": "17299946839009927524"
     }
    }
   },
   "outputs": [],
   "source": [
    "def forward(model, dl):\n",
    "    pooled = []\n",
    "    hidden = []\n",
    "    model.eval()\n",
    "    for data in tqdm.tqdm(dl):\n",
    "        with torch.no_grad():\n",
    "            data = data.to(device)\n",
    "            output = model(data, output_hidden_states=True).cpu()\n",
    "        p, h = output.pooler_output, output.last_hidden_state\n",
    "        pooled.append(p)\n",
    "        hidden.append(h[:,0,:]) # only [CLS] token embedding\n",
    "    return torch.cat(pooled), torch.cat(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "def forward_tuned(model, dl):\n",
    "    embed_l = []\n",
    "    model.eval()\n",
    "    for data in tqdm.tqdm(dl):\n",
    "        with torch.no_grad():\n",
    "            data = data.to(device)\n",
    "            embed = model(data).cpu()\n",
    "        embed = embed.last_hidden_state[:,0,:]\n",
    "        embed_l.append(embed)\n",
    "    return torch.cat(embed_l)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "361e9997-879e-4c49-a51f-e2ebbb9bdd3d",
   "metadata": {
    "id": "361e9997-879e-4c49-a51f-e2ebbb9bdd3d",
    "outputId": "5886f39d-794e-49c4-b864-2d98208bf09b",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "executionInfo": {
     "status": "error",
     "timestamp": 1680317181059,
     "user_tz": -540,
     "elapsed": 70103,
     "user": {
      "displayName": "김병권",
      "userId": "17299946839009927524"
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/293 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[82], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m forward_func \u001B[38;5;241m=\u001B[39m forward_tuned \u001B[38;5;28;01mif\u001B[39;00m load_tuned \u001B[38;5;28;01melse\u001B[39;00m forward\n\u001B[1;32m----> 2\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mforward_func\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdl\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[80], line 8\u001B[0m, in \u001B[0;36mforward\u001B[1;34m(model, dl)\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m      7\u001B[0m     data \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m----> 8\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mcpu()\n\u001B[0;32m      9\u001B[0m p, h \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39mpooler_output, output\u001B[38;5;241m.\u001B[39mlast_hidden_state\n\u001B[0;32m     10\u001B[0m pooled\u001B[38;5;241m.\u001B[39mappend(p)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mC:\\Program Files\\Python311\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:852\u001B[0m, in \u001B[0;36mRobertaModel.forward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    843\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers)\n\u001B[0;32m    845\u001B[0m embedding_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings(\n\u001B[0;32m    846\u001B[0m     input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[0;32m    847\u001B[0m     position_ids\u001B[38;5;241m=\u001B[39mposition_ids,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    850\u001B[0m     past_key_values_length\u001B[38;5;241m=\u001B[39mpast_key_values_length,\n\u001B[0;32m    851\u001B[0m )\n\u001B[1;32m--> 852\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    853\u001B[0m \u001B[43m    \u001B[49m\u001B[43membedding_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    854\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    855\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    856\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    857\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_extended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    858\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    859\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    860\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    861\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    862\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    863\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    864\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    865\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler(sequence_output) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mC:\\Program Files\\Python311\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:527\u001B[0m, in \u001B[0;36mRobertaEncoder.forward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    518\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mcheckpoint\u001B[38;5;241m.\u001B[39mcheckpoint(\n\u001B[0;32m    519\u001B[0m         create_custom_forward(layer_module),\n\u001B[0;32m    520\u001B[0m         hidden_states,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    524\u001B[0m         encoder_attention_mask,\n\u001B[0;32m    525\u001B[0m     )\n\u001B[0;32m    526\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 527\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    528\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    529\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    530\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    531\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    532\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    533\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    534\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    535\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    537\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache:\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mC:\\Program Files\\Python311\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:411\u001B[0m, in \u001B[0;36mRobertaLayer.forward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[0;32m    399\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[0;32m    400\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    401\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    408\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[0;32m    409\u001B[0m     \u001B[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001B[39;00m\n\u001B[0;32m    410\u001B[0m     self_attn_past_key_value \u001B[38;5;241m=\u001B[39m past_key_value[:\u001B[38;5;241m2\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m past_key_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 411\u001B[0m     self_attention_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    412\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    413\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    414\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    415\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    416\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mself_attn_past_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    417\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    418\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m self_attention_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    420\u001B[0m     \u001B[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mC:\\Program Files\\Python311\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:338\u001B[0m, in \u001B[0;36mRobertaAttention.forward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[0;32m    328\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[0;32m    329\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    330\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    336\u001B[0m     output_attentions: Optional[\u001B[38;5;28mbool\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    337\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[1;32m--> 338\u001B[0m     self_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    339\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    340\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    341\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    342\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    343\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    344\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    345\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    346\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    347\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput(self_outputs[\u001B[38;5;241m0\u001B[39m], hidden_states)\n\u001B[0;32m    348\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (attention_output,) \u001B[38;5;241m+\u001B[39m self_outputs[\u001B[38;5;241m1\u001B[39m:]  \u001B[38;5;66;03m# add attentions if we output them\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mC:\\Program Files\\Python311\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:218\u001B[0m, in \u001B[0;36mRobertaSelfAttention.forward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[0;32m    216\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    217\u001B[0m     key_layer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtranspose_for_scores(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkey(hidden_states))\n\u001B[1;32m--> 218\u001B[0m     value_layer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtranspose_for_scores(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalue\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m    220\u001B[0m query_layer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtranspose_for_scores(mixed_query_layer)\n\u001B[0;32m    222\u001B[0m use_cache \u001B[38;5;241m=\u001B[39m past_key_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "forward_func = forward_tuned if load_tuned else forward\n",
    "result = forward_func(model, dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([11520, 768])"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "24dda5b7-f63d-4526-9083-1851c2acba83",
   "metadata": {
    "id": "24dda5b7-f63d-4526-9083-1851c2acba83",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1680317181062,
     "user_tz": -540,
     "elapsed": 13,
     "user": {
      "displayName": "김병권",
      "userId": "17299946839009927524"
     }
    }
   },
   "outputs": [],
   "source": [
    "torch.save(result, 'roberta/tuned_embed.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
